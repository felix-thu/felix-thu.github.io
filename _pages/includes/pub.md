# üìù Publications
## Reinforcement Learning

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2025</div><img src='images/rpex.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Robust Policy Expansion for Offline-to-Online RL
under Diverse Data Corruption](https://arxiv.org/pdf/2509.24748) \\
**Longxiang He**, Deheng Ye, Junbo Tan, Xueqian Wang, Li Shen 

[**TLDR**](https://arxiv.org/pdf/2509.24748) 
  - We propose RPEX, an Offline-to-Online method that improves the performance of offline pre-
trained RL policies under a wide range of data corruptions.
</div>
</div>

[AlignIQL: Policy Alignment in Implicit Q-Learning through Constrained Optimization
](https://arxiv.org/abs/2405.18187), Arxiv 2024, **Longxiang He**, Li Shen, Junbo Tan, Xueqian Wang 
  - **TLDR:** We introduce a new method (AlignIQL) to extract the policy from the IQL-style value function
and explain when IQL can utilize weighted regression for policy extraction.

<hr>

[DiffCPS: Diffusion Model based Constrained Policy Search for Offline Reinforcement Learning](https://arxiv.org/abs/2310.05333), Arxiv 2024, **Longxiang He**, Li Shen, Linrui Zhang, Junbo Tan, Xueqian Wang 
  - **TLDR:** DiffCPS integrates diffusion-based policies into Advantage-Weighted Regression (AWR) via a
primal-dual framework and offers insights into the advantages of employing diffusion models in
offline decision-making, as well as elucidates the relationship between AWR and TD3+BC.

<hr>

[FOSP: Fine-tuning Offline Safe Policy through World Models](https://arxiv.org/abs/2407.04942), ICLR 2025,
Chenyang Cao, Yucheng Xin, Silang Wu, **Longxiang He**, Zichen Yan, Junbo Tan, Xueqian Wang

<hr>